{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81bf8a3c-ee00-447a-946a-815e6d50b7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (5.2.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (1.13.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: torch in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
      "Requirement already satisfied: anyio in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\sangeetha\\anaconda3\\lib\\site-packages (from typer-slim->transformers) (8.1.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers faiss-cpu transformers torch PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce25894-3af6-42d2-bbe5-97a65569f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import faiss\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffcd03dc-c983-42a5-94eb-483f2f2c6223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26efa046b0d74d008f8cd1c0f543edf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8956609-c479-4454-ae3b-4b2eb20ebcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION = 384\n",
    "index = faiss.IndexFlatL2(DIMENSION)\n",
    "document_chunks = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "096de6be-6a11-4383-84c5-aa3cd24ae8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bcf044e-e2f5-4eb9-b2f1-b48e42fbb435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_document(file_path):\n",
    "    global document_chunks, index\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(\"File not found. Check the path.\")\n",
    "\n",
    "    text = \"\"\n",
    "\n",
    "    if file_path.lower().endswith(\".pdf\"):\n",
    "        reader = PdfReader(file_path)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \" \"\n",
    "\n",
    "    elif file_path.lower().endswith(\".txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "    else:\n",
    "        raise ValueError(\"Only PDF or TXT files are supported.\")\n",
    "\n",
    "    chunks = chunk_text(text)\n",
    "    embeddings = embedding_model.encode(chunks)\n",
    "\n",
    "    index.add(np.array(embeddings).astype(\"float32\"))\n",
    "    document_chunks.extend(chunks)\n",
    "\n",
    "    print(f\"✅ Ingested {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0989296f-8a49-460f-80fd-4608e1b30d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_chunks(query, top_k=3):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    distances, indices = index.search(\n",
    "        np.array(query_embedding).astype(\"float32\"),\n",
    "        top_k\n",
    "    )\n",
    "    return [document_chunks[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "410a2a36-4225-40f9-8131-0d57224475df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74b6db5b14d454097711a0f16e3d5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/282 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def local_llm_answer(context, question):\n",
    "    prompt = f\"\"\"\n",
    "Use the context below to answer the question.\n",
    "If the answer is not in the context, say \"Not found in document\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcf980f5-34aa-429c-b9a9-f6227f97e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    start_time = time.time()\n",
    "\n",
    "    chunks = retrieve_chunks(question)\n",
    "    context = \"\\n\".join(chunks)\n",
    "    answer = local_llm_answer(context, question)\n",
    "\n",
    "    latency = time.time() - start_time\n",
    "    return answer, latency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cafc4ccd-6b71-4954-b258-01912cb0f4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ingested 4 chunks\n"
     ]
    }
   ],
   "source": [
    "ingest_document(r\"C:\\Users\\Sangeetha\\Downloads\\SangeethaJames_InternshalaResume.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd4d2872-aef1-4026-a978-eb71f18f0534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Python, SQL, NumPy, Pandas, Matplotlib, Seaborn, Scikit-learn, Power BI, Jupyter Notebook, VS Code, Machine Learning, Deep Learning, Natural Language Processing (NLP)\n",
      "Latency: 2.78 seconds\n"
     ]
    }
   ],
   "source": [
    "answer, latency = ask_question(\"What are my skills?\")\n",
    "print(\"Answer:\", answer)\n",
    "print(\"Latency:\", round(latency, 2), \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272a1ae-a6d4-40de-a2ce-e50f8e7ce682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcfe1d8-9a72-44a8-82e6-4fffad6ca806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
